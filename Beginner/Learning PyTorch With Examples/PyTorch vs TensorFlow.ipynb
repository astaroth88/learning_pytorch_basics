{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow: Static Graphs\n",
    "PyTorch autograd looks a lot like TensorFlow: in both frameworks we define a computational graph, and use automatic differentiation to compute gradients.\n",
    "The biggest difference betwee the to is that TensorFlow's computational graphs are static and PyTorch uses dynamic computation graphs.\n",
    "\n",
    "In TensorFlow, we define the computational graph once and then execute the same graph over and over again, possibly feeding input data to the graph. In PyTorch, each forward pass defines a new computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary packages\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we set up the computational graph:\n",
    "\n",
    "# N is batch size\n",
    "# D_in is input dimension\n",
    "# H is hidden dimension\n",
    "# D_out is output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating placefolders for the i/p and target data\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None, D_in))\n",
    "y = tf.placeholder(tf.float32, shape=(None, D_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Variables for the weights and \n",
    "# initialize them with random data.\n",
    "# A TensorFlow Variable persists its value across\n",
    "# executions of the graph\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal((D_in, H)))\n",
    "w2 = tf.Variable(tf.random_normal((H, D_out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "# Compute the predicted y using operations on TensorFlow Tensors.\n",
    "\n",
    "h = tf.matmul(x, w1)\n",
    "h_relu = tf.maximum(h, tf.zeros(1))\n",
    "y_pred = tf.matmul(h_relu, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute loss using operations on Tensorflow Tensors\n",
    "loss = tf.reduce_sum((y - y_pred) ** 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradient of the loss w.r.t w1 and w2\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the weights using gradient descent.\n",
    "# To update the weights, we need to evaluate new_w1 and new_w2\n",
    "# when executing the graph\n",
    "# Note: \n",
    "# In Tensorflow, the act of updating the value of the weights\n",
    "# is part of the computational graph;\n",
    "# In PyTorch, this happens outside the computational graph\n",
    "\n",
    "learning_rate = 1e-6 # learing rate\n",
    "\n",
    "new_w1 = w1.assign(w1 - learning_rate * grad_w1)\n",
    "new_w2 = w2.assign(w2 - learning_rate * grad_w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no of epochs\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 26856216.0\n",
      "1 22561762.0\n",
      "2 21610822.0\n",
      "3 20932736.0\n",
      "4 19044610.0\n",
      "5 15430631.0\n",
      "6 11177201.0\n",
      "7 7319739.0\n",
      "8 4573650.0\n",
      "9 2830814.5\n",
      "10 1813216.5\n",
      "11 1225071.6\n",
      "12 881036.0\n",
      "13 670022.44\n",
      "14 532796.6\n",
      "15 437559.25\n",
      "16 367538.3\n",
      "17 313462.66\n",
      "18 270213.34\n",
      "19 234721.12\n",
      "20 205090.83\n",
      "21 180031.86\n",
      "22 158664.44\n",
      "23 140342.77\n",
      "24 124536.38\n",
      "25 110832.79\n",
      "26 98876.66\n",
      "27 88413.414\n",
      "28 79225.69\n",
      "29 71129.57\n",
      "30 63976.473\n",
      "31 57643.695\n",
      "32 52022.652\n",
      "33 47022.266\n",
      "34 42561.43\n",
      "35 38579.438\n",
      "36 35019.89\n",
      "37 31828.242\n",
      "38 28959.805\n",
      "39 26377.145\n",
      "40 24049.463\n",
      "41 21948.855\n",
      "42 20050.271\n",
      "43 18332.729\n",
      "44 16776.467\n",
      "45 15364.527\n",
      "46 14082.914\n",
      "47 12917.729\n",
      "48 11857.256\n",
      "49 10891.403\n",
      "50 10011.109\n",
      "51 9207.289\n",
      "52 8473.479\n",
      "53 7802.87\n",
      "54 7189.3604\n",
      "55 6628.0723\n",
      "56 6113.5586\n",
      "57 5641.924\n",
      "58 5209.412\n",
      "59 4812.579\n",
      "60 4448.0854\n",
      "61 4113.158\n",
      "62 3805.3748\n",
      "63 3522.254\n",
      "64 3261.5608\n",
      "65 3021.539\n",
      "66 2800.333\n",
      "67 2596.374\n",
      "68 2408.2568\n",
      "69 2234.6687\n",
      "70 2074.4536\n",
      "71 1926.4451\n",
      "72 1789.784\n",
      "73 1663.3711\n",
      "74 1546.479\n",
      "75 1438.3016\n",
      "76 1338.2213\n",
      "77 1245.5511\n",
      "78 1159.7319\n",
      "79 1080.422\n",
      "80 1006.998\n",
      "81 938.8671\n",
      "82 875.66583\n",
      "83 816.9446\n",
      "84 762.40717\n",
      "85 711.7284\n",
      "86 664.6336\n",
      "87 620.8335\n",
      "88 580.0976\n",
      "89 542.1892\n",
      "90 506.9066\n",
      "91 474.0587\n",
      "92 443.4704\n",
      "93 414.96338\n",
      "94 388.392\n",
      "95 363.6266\n",
      "96 340.52505\n",
      "97 318.9771\n",
      "98 298.87057\n",
      "99 280.1085\n",
      "100 262.5941\n",
      "101 246.23134\n",
      "102 230.94672\n",
      "103 216.66461\n",
      "104 203.31236\n",
      "105 190.8368\n",
      "106 179.16603\n",
      "107 168.24507\n",
      "108 158.03099\n",
      "109 148.4678\n",
      "110 139.51834\n",
      "111 131.14105\n",
      "112 123.292206\n",
      "113 115.9373\n",
      "114 109.04564\n",
      "115 102.586716\n",
      "116 96.53046\n",
      "117 90.85428\n",
      "118 85.52526\n",
      "119 80.52799\n",
      "120 75.83969\n",
      "121 71.44036\n",
      "122 67.30742\n",
      "123 63.42769\n",
      "124 59.78144\n",
      "125 56.35724\n",
      "126 53.13954\n",
      "127 50.11537\n",
      "128 47.27259\n",
      "129 44.598305\n",
      "130 42.08552\n",
      "131 39.720047\n",
      "132 37.4949\n",
      "133 35.40036\n",
      "134 33.429523\n",
      "135 31.573992\n",
      "136 29.826618\n",
      "137 28.180206\n",
      "138 26.628973\n",
      "139 25.168411\n",
      "140 23.791676\n",
      "141 22.494066\n",
      "142 21.27069\n",
      "143 20.117323\n",
      "144 19.029022\n",
      "145 18.003899\n",
      "146 17.036055\n",
      "147 16.124104\n",
      "148 15.262626\n",
      "149 14.4495945\n",
      "150 13.682283\n",
      "151 12.957348\n",
      "152 12.272413\n",
      "153 11.625377\n",
      "154 11.014012\n",
      "155 10.436515\n",
      "156 9.8906\n",
      "157 9.374648\n",
      "158 8.886625\n",
      "159 8.424977\n",
      "160 7.988392\n",
      "161 7.575573\n",
      "162 7.184984\n",
      "163 6.815565\n",
      "164 6.465718\n",
      "165 6.1346474\n",
      "166 5.8211646\n",
      "167 5.524473\n",
      "168 5.243172\n",
      "169 4.9772773\n",
      "170 4.724959\n",
      "171 4.486214\n",
      "172 4.26021\n",
      "173 4.045665\n",
      "174 3.8424323\n",
      "175 3.6495996\n",
      "176 3.4672146\n",
      "177 3.2940178\n",
      "178 3.1301465\n",
      "179 2.974731\n",
      "180 2.8271673\n",
      "181 2.6873713\n",
      "182 2.5546925\n",
      "183 2.4287152\n",
      "184 2.3092327\n",
      "185 2.1959429\n",
      "186 2.0882795\n",
      "187 1.9860541\n",
      "188 1.888999\n",
      "189 1.7969456\n",
      "190 1.7094325\n",
      "191 1.6263437\n",
      "192 1.5474533\n",
      "193 1.4723644\n",
      "194 1.4012296\n",
      "195 1.3336115\n",
      "196 1.2692583\n",
      "197 1.208178\n",
      "198 1.150116\n",
      "199 1.0948282\n",
      "200 1.0423834\n",
      "201 0.9924639\n",
      "202 0.94518393\n",
      "203 0.9000569\n",
      "204 0.8572354\n",
      "205 0.8164328\n",
      "206 0.77754337\n",
      "207 0.7406713\n",
      "208 0.70557684\n",
      "209 0.67217475\n",
      "210 0.6403726\n",
      "211 0.6102128\n",
      "212 0.5814127\n",
      "213 0.5540854\n",
      "214 0.5280522\n",
      "215 0.50321484\n",
      "216 0.479606\n",
      "217 0.4571696\n",
      "218 0.43578017\n",
      "219 0.41540274\n",
      "220 0.39599693\n",
      "221 0.3775172\n",
      "222 0.35994932\n",
      "223 0.3432139\n",
      "224 0.32725412\n",
      "225 0.31210223\n",
      "226 0.2976394\n",
      "227 0.2838513\n",
      "228 0.27069876\n",
      "229 0.25815687\n",
      "230 0.24622591\n",
      "231 0.23485792\n",
      "232 0.224087\n",
      "233 0.21376921\n",
      "234 0.20389028\n",
      "235 0.19454247\n",
      "236 0.18558982\n",
      "237 0.17708646\n",
      "238 0.16896462\n",
      "239 0.16123015\n",
      "240 0.15386389\n",
      "241 0.14681898\n",
      "242 0.14013146\n",
      "243 0.13372026\n",
      "244 0.12765007\n",
      "245 0.12180481\n",
      "246 0.11628171\n",
      "247 0.11101042\n",
      "248 0.10596664\n",
      "249 0.10115649\n",
      "250 0.0965646\n",
      "251 0.092200816\n",
      "252 0.088006884\n",
      "253 0.08401398\n",
      "254 0.08023915\n",
      "255 0.07663652\n",
      "256 0.07316891\n",
      "257 0.06988051\n",
      "258 0.06673376\n",
      "259 0.063722156\n",
      "260 0.06084902\n",
      "261 0.058114436\n",
      "262 0.05549775\n",
      "263 0.05300955\n",
      "264 0.0506264\n",
      "265 0.048347257\n",
      "266 0.046189424\n",
      "267 0.04412464\n",
      "268 0.04215093\n",
      "269 0.040284187\n",
      "270 0.038477324\n",
      "271 0.03676342\n",
      "272 0.03513519\n",
      "273 0.033562556\n",
      "274 0.032087293\n",
      "275 0.030659318\n",
      "276 0.029293358\n",
      "277 0.027993478\n",
      "278 0.026747968\n",
      "279 0.025571989\n",
      "280 0.024445022\n",
      "281 0.023355484\n",
      "282 0.022338796\n",
      "283 0.021346394\n",
      "284 0.020417497\n",
      "285 0.019510685\n",
      "286 0.018655395\n",
      "287 0.01784256\n",
      "288 0.017052231\n",
      "289 0.016317409\n",
      "290 0.015604983\n",
      "291 0.014928344\n",
      "292 0.01427779\n",
      "293 0.013649152\n",
      "294 0.013062982\n",
      "295 0.012496213\n",
      "296 0.011955261\n",
      "297 0.011447799\n",
      "298 0.010948937\n",
      "299 0.010475468\n",
      "300 0.010029927\n",
      "301 0.009600133\n",
      "302 0.009189946\n",
      "303 0.008800603\n",
      "304 0.008428288\n",
      "305 0.008069365\n",
      "306 0.007727325\n",
      "307 0.0074045006\n",
      "308 0.0070875715\n",
      "309 0.0067950855\n",
      "310 0.0065093874\n",
      "311 0.0062349723\n",
      "312 0.0059732534\n",
      "313 0.0057213837\n",
      "314 0.005480564\n",
      "315 0.005257084\n",
      "316 0.00503869\n",
      "317 0.0048335013\n",
      "318 0.00463507\n",
      "319 0.0044464697\n",
      "320 0.0042670923\n",
      "321 0.0040924377\n",
      "322 0.0039259945\n",
      "323 0.003764843\n",
      "324 0.0036191843\n",
      "325 0.0034754288\n",
      "326 0.003337491\n",
      "327 0.0032069825\n",
      "328 0.0030771198\n",
      "329 0.0029590954\n",
      "330 0.0028437728\n",
      "331 0.0027326988\n",
      "332 0.0026283755\n",
      "333 0.0025269613\n",
      "334 0.002427993\n",
      "335 0.002336767\n",
      "336 0.0022479494\n",
      "337 0.0021650642\n",
      "338 0.0020853372\n",
      "339 0.0020073524\n",
      "340 0.0019324843\n",
      "341 0.0018621973\n",
      "342 0.001794406\n",
      "343 0.0017299221\n",
      "344 0.0016681631\n",
      "345 0.0016071635\n",
      "346 0.0015501184\n",
      "347 0.001497174\n",
      "348 0.0014435269\n",
      "349 0.0013935113\n",
      "350 0.0013446125\n",
      "351 0.0012983736\n",
      "352 0.0012536874\n",
      "353 0.0012101763\n",
      "354 0.0011692866\n",
      "355 0.0011314036\n",
      "356 0.0010960693\n",
      "357 0.0010581048\n",
      "358 0.0010224101\n",
      "359 0.0009894293\n",
      "360 0.0009587923\n",
      "361 0.00092788803\n",
      "362 0.0008973371\n",
      "363 0.0008699473\n",
      "364 0.00084213034\n",
      "365 0.0008157122\n",
      "366 0.00079023425\n",
      "367 0.00076606835\n",
      "368 0.00074236037\n",
      "369 0.0007212041\n",
      "370 0.000699924\n",
      "371 0.0006781398\n",
      "372 0.00065724505\n",
      "373 0.00063808647\n",
      "374 0.0006191624\n",
      "375 0.00060103135\n",
      "376 0.00058460224\n",
      "377 0.0005680297\n",
      "378 0.000551758\n",
      "379 0.00053567317\n",
      "380 0.0005216855\n",
      "381 0.0005071425\n",
      "382 0.00049418176\n",
      "383 0.00048090314\n",
      "384 0.00046772338\n",
      "385 0.0004549858\n",
      "386 0.000443368\n",
      "387 0.00043093815\n",
      "388 0.00041937327\n",
      "389 0.00040893597\n",
      "390 0.00039780338\n",
      "391 0.00038877048\n",
      "392 0.00037857454\n",
      "393 0.00036956693\n",
      "394 0.00036016712\n",
      "395 0.00035141912\n",
      "396 0.00034266786\n",
      "397 0.00033489964\n",
      "398 0.00032627006\n",
      "399 0.00031843636\n",
      "400 0.00030992558\n",
      "401 0.00030407993\n",
      "402 0.00029659967\n",
      "403 0.00028908162\n",
      "404 0.0002826644\n",
      "405 0.0002758476\n",
      "406 0.0002698596\n",
      "407 0.00026376237\n",
      "408 0.00025749553\n",
      "409 0.00025190215\n",
      "410 0.00024662953\n",
      "411 0.0002413642\n",
      "412 0.0002359906\n",
      "413 0.00023067003\n",
      "414 0.00022581998\n",
      "415 0.00022154297\n",
      "416 0.0002165613\n",
      "417 0.00021238578\n",
      "418 0.00020820522\n",
      "419 0.000203913\n",
      "420 0.00019957754\n",
      "421 0.0001958394\n",
      "422 0.00019163569\n",
      "423 0.00018812474\n",
      "424 0.0001838754\n",
      "425 0.00018039264\n",
      "426 0.0001771949\n",
      "427 0.00017353642\n",
      "428 0.00017061266\n",
      "429 0.00016687637\n",
      "430 0.00016335663\n",
      "431 0.00016090593\n",
      "432 0.0001580617\n",
      "433 0.00015445378\n",
      "434 0.0001515117\n",
      "435 0.00014857764\n",
      "436 0.00014625193\n",
      "437 0.00014382806\n",
      "438 0.00014118938\n",
      "439 0.00013879543\n",
      "440 0.00013641122\n",
      "441 0.00013395953\n",
      "442 0.00013181499\n",
      "443 0.00012978728\n",
      "444 0.00012730686\n",
      "445 0.00012536762\n",
      "446 0.00012276549\n",
      "447 0.0001207686\n",
      "448 0.00011896875\n",
      "449 0.00011691322\n",
      "450 0.000115138966\n",
      "451 0.00011331717\n",
      "452 0.00011150509\n",
      "453 0.000109603796\n",
      "454 0.00010807362\n",
      "455 0.000106195985\n",
      "456 0.00010390179\n",
      "457 0.00010243231\n",
      "458 0.00010084523\n",
      "459 9.9395445e-05\n",
      "460 9.781713e-05\n",
      "461 9.6496275e-05\n",
      "462 9.493625e-05\n",
      "463 9.313975e-05\n",
      "464 9.1921465e-05\n",
      "465 9.057909e-05\n",
      "466 8.928745e-05\n",
      "467 8.7686596e-05\n",
      "468 8.653423e-05\n",
      "469 8.5230524e-05\n",
      "470 8.403939e-05\n",
      "471 8.283693e-05\n",
      "472 8.1746795e-05\n",
      "473 8.026633e-05\n",
      "474 7.914474e-05\n",
      "475 7.818042e-05\n",
      "476 7.6901444e-05\n",
      "477 7.574587e-05\n",
      "478 7.4718744e-05\n",
      "479 7.344737e-05\n",
      "480 7.244169e-05\n",
      "481 7.136587e-05\n",
      "482 7.033145e-05\n",
      "483 6.9373964e-05\n",
      "484 6.842128e-05\n",
      "485 6.762315e-05\n",
      "486 6.67326e-05\n",
      "487 6.601231e-05\n",
      "488 6.5357446e-05\n",
      "489 6.442977e-05\n",
      "490 6.3539854e-05\n",
      "491 6.26957e-05\n",
      "492 6.186264e-05\n",
      "493 6.1039005e-05\n",
      "494 6.042484e-05\n",
      "495 5.944098e-05\n",
      "496 5.866137e-05\n",
      "497 5.7890167e-05\n",
      "498 5.7245376e-05\n",
      "499 5.634412e-05\n"
     ]
    }
   ],
   "source": [
    "# starting the Tensorflow session to execute the graph\n",
    "with tf.Session() as sess:\n",
    "    # Run the graph once to initialize the Variables w1 and w2\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Create numpy arrays holding the actual data\n",
    "    # for the inputs x and targets y\n",
    "    x_value = np.random.randn(N, D_in)\n",
    "    y_value = np.random.randn(N, D_out)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        # Executing multiple times;\n",
    "        # Each time it executes, we want to bind\n",
    "        # x_value to x and y_value to y (specified using feed_dict)\n",
    "        # Each time we execute the graph, we want to compute loss\n",
    "        # the values of new_w1 & new_w2 are returned as numpy arrays\n",
    "        \n",
    "        loss_value, _, _ = sess.run([loss, new_w1, new_w2],\n",
    "                                   feed_dict={x:x_value, y:y_value})\n",
    "        print(i, loss_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
